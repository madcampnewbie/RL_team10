import time
import numpy as np
from sb3_contrib import RecurrentPPO
from stable_baselines3.common.vec_env import DummyVecEnv

from wrapper import RandomMapWrapper, StallTerminationWrapper  # ğŸ” ëª¨ë“  ë˜í¼
from gymnasium.wrappers import TimeLimit
from net import MemoryExtractor

# Grid ìƒì„± ì„¤ì •
HEIGHT, WIDTH = 15, 15
WALL_P = 0.3
MUT_RATE = 0.01
PATCH = 3
MAX_STEPS = 300
K_STALL = 20

# ğŸ§© í•™ìŠµê³¼ ë™ì¼í•œ ë˜í¼ ì²´ì¸ êµ¬ì„±
def make_env():
    env = RandomMapWrapper(HEIGHT, WIDTH, WALL_P, MUT_RATE, PATCH)
    env = StallTerminationWrapper(env, k=K_STALL, max_steps=MAX_STEPS, step_penalty=-0.01)
    env = TimeLimit(env, max_episode_steps=MAX_STEPS)
    return env

# DummyVecEnv ë˜í•‘
env = DummyVecEnv([make_env])

# ëª¨ë¸ ë¡œë“œ
model = RecurrentPPO.load("./best/best_model.zip", env=env)

# í…ŒìŠ¤íŠ¸ ì—í”¼ì†Œë“œ
obs = env.reset()
lstm_state = None
done = [False]
step_count = 0

while not done[0]:
    step_count += 1
    action, lstm_state = model.predict(obs, state=lstm_state, deterministic=False)
    obs, rewards, done, info = env.step(action)
    env.envs[0].render()

# ê²°ê³¼ ì¶œë ¥
if rewards[0] > 0:
    print(f"ğŸ‰ Goal reached in {step_count} steps!")
else:
    print(f"âŒ Failed to reach goal in {step_count} steps.")

env.close()
